# ğŸ“‘ Regulatory Summarization Evaluation

This project evaluates **summarization models** on a **regulatory dataset** using standard metrics (**ROUGE-1, ROUGE-L, BERTScore**) and provides both **static plots** and an **interactive dashboard** for analysis.

---

## ğŸ“Œ Features

- Evaluate multiple models against reference summaries
- Metrics supported:

  - **ROUGE-1**
  - **ROUGE-L**
  - **BERTScore**

- Save results to CSV (`summary_eval_scores.csv`)
- Generate **EDA plots** (bar charts across models & testcases)
- Explore results in an **interactive Dash dashboard**

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ regulatory_summarization_dataset.json   # Input dataset
â”œâ”€â”€ evaluation.py                           # Main script for computing metrics
â”œâ”€â”€ eda_summary_metrics.py                  # Static visualization (matplotlib/seaborn)
â”œâ”€â”€ summary_eval_dashboard.py               # Interactive dashboard (Dash/Plotly)
â”œâ”€â”€ requirements.txt                        # Dependencies
â”œâ”€â”€ README.md                               # Documentation
â””â”€â”€ analysis/                               # Saved plots
```

---

## âš™ï¸ Installation

1. **Clone the repo**

   ```bash
   git clone git@github.com:mansikulshrestha/regulatory-summarization-eval.git
   cd regulatory-summarization-eval
   ```

2. **Create a virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ Usage

### 1. Run Evaluation

Compute ROUGE & BERTScore metrics and save results to CSV:

```bash
python evaluation.py
```

Output:

- Tabulated scores in terminal
- `summary_eval_scores.csv` file with all results

---

### 2. Generate EDA Plots

Barplots comparing models across metrics:

```bash
python eda_summary_metrics.py
```

Plots will be saved in the `analysis/` folder.

---

### 3. Launch Interactive Dashboard

Explore results dynamically with Dash:

```bash
python summary_eval_dashboard.py
```

Go to â†’ [http://127.0.0.1:8050](http://127.0.0.1:8050) in your browser.

---

## ğŸ“Š Example Output

### Tabulated Results

```
â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â••
â”‚ Testcase ID  â”‚ Model    â”‚ ROUGE-1 Score  â”‚ ROUGE-L Score  â”‚ BERTScore â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡
â”‚ 0            â”‚ model_1  â”‚ 0.55           â”‚ 0.50           â”‚ 0.84      â”‚
â”‚ 0            â”‚ model_2  â”‚ 0.62           â”‚ 0.58           â”‚ 0.86      â”‚
â”‚ 0            â”‚ model_3  â”‚ 0.42           â”‚ 0.39           â”‚ 0.77      â”‚
â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•›
```

### Dashboard Screenshot

---

## ğŸ“Œ Future Improvements

- Add more evaluation metrics (BLEU, METEOR, etc.)
- Deploy dashboard on Render/Streamlit Cloud
- Extend dataset for other regulatory bodies

---

## ğŸ› ï¸ Tech Stack

- **Python** (pandas, numpy)
- **Metrics**: `rouge-score`, `bert-score`
- **Visualization**: matplotlib, seaborn, plotly, dash
- **Utils**: tqdm, tabulate

---

## ğŸ‘¤ Author

**Mansi Kulshrestha**

ğŸ”— [GitHub](https://github.com/mansikulshrestha) | [LinkedIn](https://www.linkedin.com/in/mansikulshrestha)
